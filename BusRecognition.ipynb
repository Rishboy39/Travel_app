{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "CSX2R4FX2C_h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)\n"
      ],
      "metadata": {
        "id": "-nVYX_q-64LO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ],
      "metadata": {
        "id": "tg7D0J1G66ud"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing checkpoints, bugging\n",
        "!rm -rf /content/data/train/.ipynb_checkpoints\n",
        "!rm -rf /content/data/val/.ipynb_checkpoints\n"
      ],
      "metadata": {
        "id": "WyHQ121m4pku"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_data = datasets.ImageFolder('/content/data/train', transform=transform)\n",
        "val_data = datasets.ImageFolder('/content/data/val', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"✅ Loaded {len(train_data)} training images and {len(val_data)} validation images.\")\n"
      ],
      "metadata": {
        "id": "_A8Jmj656911",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd23868-57a0-43ef-bc20-0bf3e83efa6a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 171 training images and 223 validation images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "sMvhszL77nwV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()  #\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}, Accuracy: {100 * correct / total}%\")\n"
      ],
      "metadata": {
        "id": "qpxn587d7r_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b81057-f855-4b07-a585-1302168fb018"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.3447714665283759, Accuracy: 91.2280701754386%\n",
            "Epoch 2/10, Loss: 0.2944112842281659, Accuracy: 95.32163742690058%\n",
            "Epoch 3/10, Loss: 0.1692212506507834, Accuracy: 92.98245614035088%\n",
            "Epoch 4/10, Loss: 0.15554529055953026, Accuracy: 95.32163742690058%\n",
            "Epoch 5/10, Loss: 0.12380111093322436, Accuracy: 95.90643274853801%\n",
            "Epoch 6/10, Loss: 0.09528650219241779, Accuracy: 96.49122807017544%\n",
            "Epoch 7/10, Loss: 0.11374134135742982, Accuracy: 97.6608187134503%\n",
            "Epoch 8/10, Loss: 0.0922712329775095, Accuracy: 97.6608187134503%\n",
            "Epoch 9/10, Loss: 0.2548397894327839, Accuracy: 94.15204678362574%\n",
            "Epoch 10/10, Loss: 0.12430937153597672, Accuracy: 96.49122807017544%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Validation Accuracy: {100 * correct / total}%\")"
      ],
      "metadata": {
        "id": "oR0vZPQa7vM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe0b2a8-b69e-4945-e806-5bb6a7ca53ff"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 86.99551569506727%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "img = Image.open('/content/pic2.jpg')\n",
        "\n",
        "img_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(img_tensor)\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "if predicted.item() == 0:\n",
        "    print(\"Bus detected!\")\n",
        "else:\n",
        "    print(\"Not a bus!\")\n"
      ],
      "metadata": {
        "id": "AVsmrLcx7yUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2af32bf-ea0a-4012-b017-b6a0cc799a0d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not a bus!\n"
          ]
        }
      ]
    }
  ]
}